{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import numpy as np\n",
    "#import rake_nltk\n",
    "root = Tk()\n",
    "my_filetypes = [('all files', '.*'), ('text files', '.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.filename =  filedialog.askopenfilename(\n",
    "                                    title=\"D:\\summary.txt\",\n",
    "                                    filetypes=my_filetypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File directory: D:/summary.txt\n",
      "\n",
      "\n",
      "Johannesburg: South Africa survived a nail-biting finish to edge out India by five wickets in a rain-curtailed fourth One-Day International (ODI) in Johannesburg on Saturday.\n",
      "\n",
      "India, who had won the first three ODIs, now have a 3-1 lead in the six-match series.\n",
      "\n",
      "Electing to bat first, India rode on opener Shikhar Dhawan's 13th ODI century to post 289/7 in their 50 overs.\n",
      "\n",
      "Dhawan scored 109 runs off 105 balls, and shared a 158-run stand with skipper Virat Kohli who scored 75.\n",
      "\n",
      "The match was interrupted by rain and inclement weather on more than one occasion which saw the target being reduced to 202 runs off 28 overs for South Africa.\n",
      "\n",
      "The Indian bowlers dominated South African batsmen in the initial stages.\n",
      "\n",
      "With their top four batsmen back in the pavilion by the 17th over with just 102 runs on the board, the Proteas seemed to have lost their way and headed for certain defeat.\n",
      "\n",
      "When David Miller was bowled while trying to hit an incoming delivery from leg-spinner Yuzvendra Chahal, the visitors must have thought that the match was as good as in the bag.\n",
      "\n",
      "But miraculously for South Africa, television replays revealed that the delivery was a no-ball and Miller was back in the middle. That turned out to be the turning point as the match quickly turned on its head.\n",
      "\n",
      "Miller celebrated by smashing pacer Hardik Pandya for three consecutive boundaries in the next over and the momentum abruptly turned in the hosts' favour.\n",
      "\n",
      "Chahal did manage to dismiss Miller eventually, trapping him leg before.\n",
      "\n",
      "But Miller, who scored 39 runs off 28 balls with four boundaries and two sixes, had turned the momentum decisively South Africa's way.\n",
      "\n",
      "Heinrich Klaasen continued the good work, plundering an unbeaten 43 off 27 deliveries with the help of five boundaries and a six.\n",
      "\n",
      "Andile Phehlukwayo was also on fire at the other end, smashing three sixes his way to a whirlwind 23 off just five balls as the hosts celebrated a well deserved victory.\n",
      "\n",
      "Chinaman Kuldeep Yadav was the most successful among the Indian bowlers with figures of 2/51 while fast bowler Jasprit Bumrah, Pandya and Chahal got a wicket each.\n",
      "\n",
      "Earlier, Dhawan shared a 158-run second-wicket stand with Kohli after opener Rohit Sharma was dismissed early on.\n",
      "\n",
      "India were cruising at 197/2 in 34 overs when play was interrupted for nearly an hour due to lightning. After play resumed, India lost the plot and lost wickets regularly to what looked an under-par total.\n",
      "\n",
      "Before the interruption, however, Dhawan and Kohli, dominated the home bowlers and steered India to a position of strength.\n",
      "\n",
      "Kohli stamped his authority as he continued to punish the South African bowlers. For someone coming into the match with scores of 112, 46 not out, 160 not out, batting looked easy. He registered his 46th ODI fifty.\n",
      "\n",
      "South Africa, playing without any spinners, looked out of ideas to stop the two with no variety in their bowling. Part-time Jean-Paul Duminy delivered four overs of spin but, expectedly, didn't look like taking a wicket.\n",
      "\n",
      "The stand was broken when Kohli hit pacer Chris Morris straight into the hands of David Miller at short cover, with India at 178/2 in 31.1 overs.\n",
      "\n",
      "India's march was halted by lightening with dark clouds hovering over the New Wanderers Stadium.\n",
      "\n",
      "When the match resumed, Dhawan was the first to be dismissed, driving Morne Morkel to AB De Villiers at mid-off.\n",
      "\n",
      "Ajinkya Rahane (8) too perished, with a pull off Lungi Ngidi failing to clear Rabada at deep square leg. With these two wickets, things suddenly looked gloomy for India, who were at 210/4.\n",
      "\n",
      "The saviour of India was old war-horse, Mahendra Singh Dhoni, who remained not out 42 off 43 deliveries.\n",
      "\n",
      "At the other end, Ngidi and Rabada accounted for Shreyas Iyer (18) and Hardik Pandya (9) to dent India's hopes of a late surge but Dhoni once again showed why he is so important for the side.\n",
      "\n",
      "\n",
      "Sentences: ['Johannesburg: South Africa survived a nail-biting finish to edge out India by five wickets in a rain-curtailed fourth One-Day International (ODI) in Johannesburg on Saturday.', 'India, who had won the first three ODIs, now have a 3-1 lead in the six-match series.', \"Electing to bat first, India rode on opener Shikhar Dhawan's 13th ODI century to post 289/7 in their 50 overs.\", 'Dhawan scored 109 runs off 105 balls, and shared a 158-run stand with skipper Virat Kohli who scored 75.', 'The match was interrupted by rain and inclement weather on more than one occasion which saw the target being reduced to 202 runs off 28 overs for South Africa.', 'The Indian bowlers dominated South African batsmen in the initial stages.', 'With their top four batsmen back in the pavilion by the 17th over with just 102 runs on the board, the Proteas seemed to have lost their way and headed for certain defeat.', 'When David Miller was bowled while trying to hit an incoming delivery from leg-spinner Yuzvendra Chahal, the visitors must have thought that the match was as good as in the bag.', 'But miraculously for South Africa, television replays revealed that the delivery was a no-ball and Miller was back in the middle.', 'That turned out to be the turning point as the match quickly turned on its head.', \"Miller celebrated by smashing pacer Hardik Pandya for three consecutive boundaries in the next over and the momentum abruptly turned in the hosts' favour.\", 'Chahal did manage to dismiss Miller eventually, trapping him leg before.', \"But Miller, who scored 39 runs off 28 balls with four boundaries and two sixes, had turned the momentum decisively South Africa's way.\", 'Heinrich Klaasen continued the good work, plundering an unbeaten 43 off 27 deliveries with the help of five boundaries and a six.', 'Andile Phehlukwayo was also on fire at the other end, smashing three sixes his way to a whirlwind 23 off just five balls as the hosts celebrated a well deserved victory.', 'Chinaman Kuldeep Yadav was the most successful among the Indian bowlers with figures of 2/51 while fast bowler Jasprit Bumrah, Pandya and Chahal got a wicket each.', 'Earlier, Dhawan shared a 158-run second-wicket stand with Kohli after opener Rohit Sharma was dismissed early on.', 'India were cruising at 197/2 in 34 overs when play was interrupted for nearly an hour due to lightning.', 'After play resumed, India lost the plot and lost wickets regularly to what looked an under-par total.', 'Before the interruption, however, Dhawan and Kohli, dominated the home bowlers and steered India to a position of strength.', 'Kohli stamped his authority as he continued to punish the South African bowlers.', 'For someone coming into the match with scores of 112, 46 not out, 160 not out, batting looked easy.', 'He registered his 46th ODI fifty.', 'South Africa, playing without any spinners, looked out of ideas to stop the two with no variety in their bowling.', \"Part-time Jean-Paul Duminy delivered four overs of spin but, expectedly, didn't look like taking a wicket.\", 'The stand was broken when Kohli hit pacer Chris Morris straight into the hands of David Miller at short cover, with India at 178/2 in 31.1 overs.', \"India's march was halted by lightening with dark clouds hovering over the New Wanderers Stadium.\", 'When the match resumed, Dhawan was the first to be dismissed, driving Morne Morkel to AB De Villiers at mid-off.', 'Ajinkya Rahane (8) too perished, with a pull off Lungi Ngidi failing to clear Rabada at deep square leg.', 'With these two wickets, things suddenly looked gloomy for India, who were at 210/4.', 'The saviour of India was old war-horse, Mahendra Singh Dhoni, who remained not out 42 off 43 deliveries.', \"At the other end, Ngidi and Rabada accounted for Shreyas Iyer (18) and Hardik Pandya (9) to dent India's hopes of a late surge but Dhoni once again showed why he is so important for the side.\"]\n",
      "\n",
      "\n",
      "bitokens feature vector: [16, 7, 13, 11, 13, 7, 14, 15, 10, 6, 14, 7, 13, 11, 16, 17, 12, 9, 10, 10, 7, 9, 3, 10, 12, 16, 9, 11, 12, 7, 8, 18]\n",
      "\n",
      "\n",
      "tritokens feature vector: [15, 6, 12, 10, 12, 6, 13, 14, 9, 5, 13, 6, 12, 10, 15, 16, 11, 8, 9, 9, 6, 8, 2, 9, 11, 15, 8, 10, 11, 6, 7, 17]\n",
      "\n",
      "\n",
      "sentence position: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "\n",
      "\n",
      "Total number of sentences: 32\n",
      "\n",
      "\n",
      "Sentence position feature vector: [1, 0.96875, 0.9375, 0.90625, 0.875, 0.84375, 0.8125, 0.78125, 0.75, 0.71875, 0.6875, 0.65625, 0.625, 0.59375, 0.5625, 0.53125, 0.5, 0.46875, 0.4375, 0.40625, 0.375, 0.34375, 0.3125, 0.28125, 0.25, 0.21875, 0.1875, 0.15625, 0.125, 0.09375, 0.0625, 1]\n",
      "\n",
      "\n",
      "SentenceVectors: [[8, 0, 2, 10, 0, 0, 0, 16, 4, 0, 0, 0, 5, 3, 3, 4, 0, 0, 14, 0, 1, 15, 0, 10, 1, 0, 5, 0, 3, 0, 24, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0, 1, 0, 1, 1, 0, 5, 0, 0, 1, 0, 2, 3, 0, 0, 3, 1, 2, 0, 2, 4], [3, 0, 0, 5, 0, 0, 0, 4, 1, 1, 0, 0, 7, 1, 0, 0, 0, 0, 6, 0, 1, 5, 0, 8, 0, 0, 5, 0, 2, 0, 16, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 0], [7, 0, 0, 9, 0, 0, 1, 7, 1, 0, 1, 0, 5, 1, 1, 1, 1, 2, 5, 0, 1, 6, 0, 7, 0, 1, 4, 0, 2, 1, 19, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 1, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 2, 1, 1, 1], [5, 0, 0, 3, 1, 0, 0, 5, 2, 0, 0, 0, 5, 3, 0, 1, 1, 2, 8, 0, 1, 4, 0, 4, 0, 1, 7, 0, 0, 1, 18, 2, 3, 2, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 7, 0, 3, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0], [10, 0, 0, 11, 0, 0, 3, 11, 4, 3, 0, 0, 8, 1, 1, 2, 0, 1, 10, 1, 1, 7, 0, 15, 0, 0, 5, 0, 0, 1, 28, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 4, 0, 0, 1, 0, 4, 0, 0, 0, 0, 1, 1, 0, 0, 7, 0, 0, 0, 0, 2], [3, 0, 0, 6, 0, 0, 0, 7, 1, 2, 0, 0, 3, 2, 0, 2, 0, 0, 6, 1, 1, 7, 0, 6, 0, 0, 4, 0, 1, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [10, 1, 0, 17, 0, 0, 1, 7, 3, 2, 0, 0, 11, 2, 2, 4, 0, 2, 11, 0, 1, 8, 0, 18, 0, 1, 6, 0, 0, 0, 32, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 9, 1, 2, 0, 0, 0, 0, 6, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0], [8, 0, 0, 13, 0, 0, 0, 9, 1, 4, 0, 0, 13, 7, 2, 2, 0, 1, 13, 0, 1, 12, 1, 13, 0, 0, 8, 0, 0, 0, 30, 3, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 7, 0, 4, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 5, 0, 0, 2, 0, 1, 1, 0, 6], [5, 0, 0, 7, 0, 0, 0, 4, 2, 2, 0, 0, 4, 11, 3, 2, 0, 1, 11, 0, 1, 8, 1, 12, 0, 0, 5, 0, 0, 0, 20, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 2, 0, 0, 1, 0, 5, 0, 0, 1, 0, 1, 3, 0, 1, 3, 0, 0, 1, 0, 0], [4, 0, 0, 11, 0, 0, 0, 6, 0, 1, 0, 0, 5, 1, 1, 1, 0, 1, 4, 1, 1, 4, 0, 6, 0, 0, 2, 0, 0, 0, 15, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1], [7, 0, 0, 11, 0, 0, 0, 10, 2, 4, 0, 0, 6, 4, 3, 4, 0, 2, 10, 0, 1, 7, 1, 17, 0, 0, 6, 1, 0, 0, 23, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 11, 1, 0, 0, 0, 0, 0, 6, 0, 0, 1, 1, 0, 3, 0, 0, 4, 0, 0, 0, 0, 1], [2, 0, 0, 3, 0, 0, 0, 3, 1, 3, 0, 0, 3, 6, 1, 1, 0, 2, 6, 0, 1, 6, 1, 7, 0, 0, 3, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 3], [8, 0, 0, 7, 0, 0, 1, 5, 4, 3, 0, 0, 5, 5, 2, 2, 1, 0, 6, 0, 1, 7, 1, 9, 0, 0, 8, 0, 0, 1, 22, 7, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 7, 0, 4, 0, 0, 1, 0, 6, 0, 0, 0, 1, 1, 1, 0, 1, 3, 0, 0, 2, 0, 0], [7, 0, 0, 5, 0, 0, 1, 11, 4, 0, 0, 1, 5, 4, 0, 2, 0, 2, 7, 0, 1, 10, 0, 14, 0, 1, 4, 1, 0, 0, 21, 4, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 5, 0, 2, 0, 0, 0, 0, 6, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 2], [8, 1, 0, 10, 0, 0, 1, 5, 4, 1, 0, 0, 10, 9, 3, 2, 0, 0, 11, 0, 1, 9, 0, 18, 0, 0, 13, 0, 0, 0, 30, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 7, 1, 6, 0, 0, 1, 0, 6, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 1, 0, 1], [6, 0, 1, 8, 0, 0, 1, 7, 4, 4, 1, 0, 9, 6, 1, 2, 0, 2, 17, 0, 1, 7, 0, 11, 0, 0, 9, 0, 1, 0, 26, 5, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 5, 1, 6, 0, 0, 0, 0, 5, 0, 2, 1, 0, 0, 1, 0, 1, 4, 0, 0, 1, 0, 3], [5, 0, 0, 5, 0, 1, 0, 6, 1, 2, 0, 0, 6, 3, 1, 0, 0, 1, 11, 0, 1, 7, 0, 9, 0, 0, 7, 0, 0, 1, 16, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 8, 0, 4, 0, 0, 0, 1, 5, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0], [4, 0, 0, 5, 0, 0, 1, 9, 1, 0, 1, 1, 3, 3, 2, 0, 1, 2, 6, 0, 1, 7, 0, 8, 0, 1, 3, 0, 1, 0, 18, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 8, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3], [7, 0, 0, 10, 0, 0, 0, 4, 1, 1, 0, 0, 2, 8, 2, 0, 0, 3, 8, 0, 1, 2, 0, 8, 0, 0, 4, 0, 1, 0, 16, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 2, 0, 0, 1, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], [11, 0, 0, 10, 0, 0, 0, 9, 2, 2, 0, 0, 7, 2, 0, 1, 0, 2, 7, 0, 1, 7, 0, 14, 0, 0, 4, 0, 1, 0, 18, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 3, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 3, 0, 1], [6, 0, 0, 7, 0, 0, 0, 4, 1, 1, 0, 0, 7, 2, 1, 1, 0, 2, 4, 0, 1, 6, 0, 5, 0, 0, 5, 0, 0, 0, 12, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0], [13, 0, 0, 10, 0, 0, 1, 6, 1, 3, 0, 1, 3, 1, 1, 1, 0, 0, 3, 0, 1, 4, 0, 6, 0, 0, 4, 0, 0, 0, 18, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 2], [0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 3, 0, 4, 0, 0, 2, 1, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1], [11, 0, 0, 11, 0, 0, 0, 7, 2, 0, 0, 0, 5, 3, 3, 1, 0, 3, 5, 0, 1, 10, 0, 6, 0, 0, 4, 0, 0, 0, 19, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 2], [5, 0, 1, 7, 0, 0, 0, 5, 2, 2, 0, 0, 0, 5, 2, 1, 0, 2, 5, 0, 1, 8, 0, 11, 0, 0, 2, 0, 0, 0, 15, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 4, 2, 1, 0, 0, 0, 0, 5, 0, 0, 4, 1, 0, 2, 0, 0, 2, 0, 1, 2, 0, 1], [8, 0, 0, 10, 0, 0, 1, 7, 1, 0, 1, 0, 10, 3, 0, 1, 0, 1, 9, 1, 2, 11, 2, 8, 0, 1, 8, 0, 1, 1, 26, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 10, 0, 3, 0, 0, 0, 0, 4, 0, 1, 1, 0, 0, 3, 0, 0, 2, 0, 1, 1, 0, 1], [3, 0, 0, 5, 0, 0, 0, 5, 0, 2, 0, 0, 6, 3, 1, 1, 0, 0, 7, 0, 1, 6, 0, 8, 0, 0, 4, 0, 1, 0, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 6, 0, 3, 0, 0, 0, 0, 6, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 0, 0, 3], [5, 0, 0, 7, 1, 0, 0, 4, 3, 4, 0, 0, 5, 3, 0, 1, 0, 0, 5, 0, 1, 8, 2, 11, 0, 0, 7, 0, 0, 0, 19, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 6, 0, 2, 0, 0, 1, 0, 5, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 1], [4, 1, 0, 4, 0, 2, 0, 4, 3, 0, 0, 0, 3, 5, 1, 1, 0, 3, 11, 0, 1, 8, 0, 8, 1, 0, 2, 0, 0, 1, 18, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 1, 0, 4, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 4], [7, 0, 0, 6, 0, 0, 1, 3, 1, 1, 1, 1, 4, 3, 2, 0, 0, 0, 2, 0, 1, 4, 0, 7, 0, 0, 4, 0, 1, 0, 13, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2], [9, 0, 0, 2, 0, 0, 1, 6, 3, 1, 0, 2, 6, 2, 0, 0, 0, 0, 7, 1, 1, 7, 1, 8, 0, 0, 4, 0, 1, 0, 17, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 0, 1], [12, 0, 0, 11, 0, 1, 0, 11, 3, 1, 0, 0, 9, 1, 4, 2, 1, 2, 16, 0, 1, 9, 0, 15, 2, 0, 8, 1, 2, 1, 36, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 8, 1, 2, 0, 0, 1, 0, 12, 0, 0, 1, 1, 1, 0, 0, 0, 3, 2, 1, 1, 0, 3]]\n",
      "\n",
      "\n",
      "TF-ISF vector: [0.27786296577622877, 0.1354430630235733, 0.14769386498183543, 0.17993568342364608, 0.20181437824707543, 0.11168607408565301, 0.21477752625876345, 0.25495427579374486, 0.16236717500537828, 0.14154942076647511, 0.20828914562633108, 0.11380892646994474, 0.17001948541498257, 0.17053865597923004, 0.20977919853437338, 0.20979347741178522, 0.16659023639061846, 0.14575923892314707, 0.13490597443213878, 0.1690303785932545, 0.11797914754700223, 0.19589361688787083, 0.12133987652132812, 0.14871749221543074, 0.1536091227767305, 0.18234449152654128, 0.1433508418601759, 0.16248633942511198, 0.21343122001787124, 0.12098452712125364, 0.1472543046919499, 0.23867985546515974]\n",
      "\n",
      "\n",
      "Max TF-ISF: [0.27786296577622877]\n",
      "\n",
      "\n",
      "Centroid: [13, 0, 0, 10, 0, 0, 1, 6, 1, 3, 0, 1, 3, 1, 1, 1, 0, 0, 3, 0, 1, 4, 0, 6, 0, 0, 4, 0, 0, 0, 18, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 2]\n",
      "\n",
      "\n",
      "Cosine Similarity Vector: [0.8319728505789191, 0.8486311686207253, 0.9187507022860952, 0.8296241218379459, 0.9032773501732787, 0.8418351085701967, 0.9035524492725449, 0.874632841757259, 0.8072133664443875, 0.8997331953901757, 0.8580978007702723, 0.7666132016217918, 0.88612908914655, 0.8544619213323813, 0.8498113471991462, 0.8390413163568903, 0.8162757901985805, 0.8457127543368522, 0.8644479072006889, 0.9010432754095755, 0.89407604097742, 0.9999999999999999, 0.7150553021809394, 0.9359031568741178, 0.8422427656183828, 0.8800573444886922, 0.8152631880432277, 0.8684523125448452, 0.8041227180170499, 0.925516812826188, 0.8648902265850976, 0.88784877071642]\n",
      "\n",
      "\n",
      "Sentence length feature vector: [0.8947368421052632, 0.42105263157894735, 0.7368421052631579, 0.631578947368421, 0.7368421052631579, 0.42105263157894735, 0.7894736842105263, 0.8421052631578947, 0.5789473684210527, 0.3684210526315789, 0.7894736842105263, 0.42105263157894735, 0.7368421052631579, 0.631578947368421, 0.8947368421052632, 0.9473684210526315, 0.6842105263157895, 0.5263157894736842, 0.5789473684210527, 0.5789473684210527, 0.42105263157894735, 0.5263157894736842, 0.21052631578947367, 0.5789473684210527, 0.6842105263157895, 0.8947368421052632, 0.5263157894736842, 0.631578947368421, 0.6842105263157895, 0.42105263157894735, 0.47368421052631576, 1.0]\n",
      "\n",
      "\n",
      "Numeric token feature vector: [0.0, 0.25, 0.2857142857142857, 0.3333333333333333, 0.14285714285714285, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.16666666666666666, 0.058823529411764705, 0.1111111111111111, 0.07692307692307693, 0.3, 0.0, 0.0, 0.0, 0.3, 0.25, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.07692307692307693, 0.25, 0.2222222222222222, 0.10526315789473684]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rake_nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7d94f20e4572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;31m# # Thematic words feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrake_nltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Uses stopwords for english from NLTK, and all puntuation characters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rake_nltk'"
     ]
    }
   ],
   "source": [
    "print (\"File directory:\",root.filename)\n",
    "print(\"\\n\")\n",
    "root.withdraw()\n",
    "#text=open(root.filename, encoding=\"utf-8\").read()\n",
    "text=open(root.filename).read()\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence segmentation\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sentences=(sent_tokenize(text))\n",
    "print(\"Sentences:\",sentences)\n",
    "print(\"\\n\")\n",
    "#print(len(sentences))\n",
    "\n",
    "\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "\n",
    "\n",
    "# # Tokenization, Stop word removal , Bi-grams, Tri-grams\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    \n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "print(\"bitokens feature vector:\",(bi_token_length))\n",
    "#print(max(bi_token_length))\n",
    "#print(bi_token_length)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "\n",
    "print(\"tritokens feature vector:\",tri_token_length)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "num_sent=len(sent_position)\n",
    "print(\"sentence position:\",sent_position)\n",
    "print(\"\\n\")\n",
    "print(\"Total number of sentences:\",num_sent)\n",
    "print(\"\\n\")\n",
    "#th= 0.2*num_sent\n",
    "#minv=th*num_sent\n",
    "#maxv=th*2*num_sent\n",
    "position = []\n",
    "position_rbm = []\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "for x in range(1,num_sent-1):\n",
    "    #s_p = (math.cos((sent_position[x]-minv)*((1/maxv)-minv)))*100\n",
    "    #if s_p < 0:\n",
    "     #   s_p = 0\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "sent_pos2 = 100\n",
    "sent_pos2_rbm = 1\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "print(\"Sentence position feature vector:\",position_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "print(\"SentenceVectors:\",VSM)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TF-ISF feature and Centroid Calculation\n",
    "\n",
    "sentencelength=len(sentences)\n",
    "def calcMeanTF_ISF(VSM, index):\n",
    "    vocab_len = len(VSM[index])\n",
    "    sentences_len = len(VSM)\n",
    "    count = 0\n",
    "    tfisf = 0\n",
    "    for i in range(vocab_len):\n",
    "        tf = VSM[index][i]\n",
    "        if(tf>0):\n",
    "            count += 1\n",
    "            sent_freq = 0\n",
    "            for j in range(sentences_len):\n",
    "                if(VSM[j][i]>0): sent_freq += 1\n",
    "            tfisf += (tf)*(1.0/sent_freq)\n",
    "    if(count > 0):\n",
    "        mean_tfisf = tfisf/count\n",
    "    else:\n",
    "        mean_tfisf = 0\n",
    "    return tf, (1.0/sent_freq), mean_tfisf\n",
    "tfvec=[]\n",
    "isfvec=[]\n",
    "tfisfvec=[]\n",
    "tfisfvec_rbm=[]\n",
    "for i in range(sentencelength):\n",
    "    x,y,z=calcMeanTF_ISF(VSM,i)\n",
    "    tfvec.append(x)\n",
    "    isfvec.append(y)\n",
    "    tfisfvec.append(z*100)\n",
    "    tfisfvec_rbm.append(z)\n",
    "#print(\"TF vector:\",tfvec)\n",
    "#print(\"\\n\")\n",
    "#print(\"ISF vector:\",isfvec)\n",
    "#print(\"\\n\")\n",
    "#tfisf1= [(int(p)*100) for p in tfisfvec]\n",
    "print(\"TF-ISF vector:\",tfisfvec_rbm)\n",
    "print(\"\\n\")\n",
    "maxtf_isf=max(tfisfvec_rbm)\n",
    "centroid=[]\n",
    "centroid.append(maxtf_isf)\n",
    "print(\"Max TF-ISF:\",centroid)\n",
    "print(\"\\n\")\n",
    "#for q in range(sentencelength):\n",
    "centroid=(max(VSM))\n",
    "print(\"Centroid:\",centroid)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Cosine Similarity between Centroid and Sentences\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine_similarity=[]\n",
    "cosine_similarity_rbm=[]\n",
    "for z in range(sentencelength):\n",
    "    cos_simi = ((dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))*100)\n",
    "    cosine_similarity.append(cos_simi)\n",
    "    cos_simi_rbm = (dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))\n",
    "    cosine_similarity_rbm.append(cos_simi_rbm)\n",
    "print(\"Cosine Similarity Vector:\",cosine_similarity_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence length feature\n",
    "\n",
    "sent_word=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    a=(len(sent_split))\n",
    "    sent_word.append(a)\n",
    "#print(\"Number of words in each sentence:\",sent_word)\n",
    "#print(\"\\n\")\n",
    "#sent_leng=[]\n",
    "#for x in range(len(sentences)):\n",
    " #   if sent_word[x] < 3:\n",
    "  #      sent_leng.append(0)\n",
    "  #  else:\n",
    "   #     sent_leng.append(1)\n",
    "\n",
    "##OR BY THIS METHOD: LENGTH OF SENTENCE/ LONGEST SENTENCE\n",
    "longest_sent=max(sent_word)\n",
    "sent_length=[]\n",
    "sent_length_rbm=[]\n",
    "for x in sent_word:\n",
    "    sent_length.append((x/longest_sent)*100)\n",
    "    sent_length_rbm.append(x/longest_sent)\n",
    "#print(sent_length)\n",
    "\n",
    "print(\"Sentence length feature vector:\",sent_length_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Numeric token Feature\n",
    "\n",
    "import re\n",
    "num_word=[]\n",
    "numeric_token=[]\n",
    "numeric_token_rbm=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split4=sentences[u].split(\" \")\n",
    "    e=re.findall(\"\\d+\",sentences[u])\n",
    "    noofwords=(len(e))\n",
    "    num_word.append(noofwords)\n",
    "    numeric_token.append((num_word[u]/sent_word[u])*100)\n",
    "    numeric_token_rbm.append(num_word[u]/sent_word[u])\n",
    "#print(\"Numeric word count in each sentence:\",num_word)\n",
    "#print(\"\\n\")\n",
    "print(\"Numeric token feature vector:\",numeric_token_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Thematic words feature\n",
    "\n",
    "from rake_nltk import Rake\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "keywords=[]\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "# r = Rake(<list of stopwords>, <string of puntuations to ignore>)\n",
    "\n",
    "for s in sentences:\n",
    "    r.extract_keywords_from_text(s)\n",
    "    key=list(r.get_ranked_phrases())\n",
    "    keywords.append(key)\n",
    "#print(keywords)\n",
    "l_keywords=[]\n",
    "for s in keywords:\n",
    "    leng=len(s)\n",
    "    l_keywords.append(leng)\n",
    "#print(l_keywords)\n",
    "\n",
    "total_keywords=sum(l_keywords)\n",
    "#print(total_keywords)\n",
    "\n",
    "thematic_number= []\n",
    "thematic_number_rbm= []\n",
    "for x in l_keywords:\n",
    "    thematic_number.append((x/total_keywords)*100)\n",
    "    thematic_number_rbm.append(x/total_keywords)\n",
    "print(\"Thematic word feature\", thematic_number_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # proper noun feature\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "pncounts = []\n",
    "pncounts_rbm = []\n",
    "for sentence in sentences:\n",
    "    tagged=nltk.pos_tag(nltk.word_tokenize(str(sentence)))\n",
    "    counts = Counter(tag for word,tag in tagged if tag.startswith('NNP') or tag.startswith('NNPS'))\n",
    "    f=sum(counts.values())\n",
    "    pncounts.append(f)\n",
    "    pncounts_rbm.append(f)\n",
    "pnounscore=[(int(o) / int(p))*100 for o,p in zip(pncounts, sent_word)]\n",
    "pnounscore_rbm=[int(o) / int(p) for o,p in zip(pncounts_rbm, sent_word)]\n",
    "#print(pncounts)\n",
    "print(\"Pronoun feature vector\",pnounscore_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# # feature matrix1\n",
    "\n",
    "\n",
    "featureMatrix = []\n",
    "featureMatrix.append(position_rbm)\n",
    "featureMatrix.append(bi_token_length)\n",
    "featureMatrix.append(tri_token_length)\n",
    "featureMatrix.append(tfisfvec_rbm)\n",
    "featureMatrix.append(cosine_similarity_rbm)\n",
    "featureMatrix.append(thematic_number_rbm)\n",
    "featureMatrix.append(sent_length_rbm)\n",
    "featureMatrix.append(numeric_token_rbm)\n",
    "featureMatrix.append(pnounscore_rbm)\n",
    "\n",
    "\n",
    "\n",
    "featureMat = np.zeros((len(sentences),9))\n",
    "for i in range(9) :\n",
    "    for j in range(len(sentences)):\n",
    "        featureMat[j][i] = featureMatrix[i][j]\n",
    "\n",
    "print(\"\\n\\n\\nPrinting Feature Matrix : \")\n",
    "print(featureMat)\n",
    "print(\"\\n\\n\\nPrinting Feature Matrix Normed : \")\n",
    "#featureMat_normed = featureMat / featureMat.max(axis=0)\n",
    "featureMat_normed = featureMat\n",
    "\n",
    "print(featureMat_normed)\n",
    "for i in range(len(sentences)):\n",
    "    print(featureMat_normed[i])\n",
    "#np.save('output_labels_10.npy',featureMat_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# New Antecedent/Consequent objects hold universe variables and membership\n",
    "# functions\n",
    "position1 = ctrl.Antecedent(np.arange(0, 100, 10), 'position1')\n",
    "cos_similarity = ctrl.Antecedent(np.arange(0, 100, 10), 'cos_similarity')\n",
    "bitokens = ctrl.Antecedent(np.arange(0, 100, 10), 'bitokens')\n",
    "tritokens = ctrl.Antecedent(np.arange(0, 100, 10), 'tritokens')\n",
    "propernoun = ctrl.Antecedent(np.arange(0, 100, 10), 'propernoun')\n",
    "sentencelength = ctrl.Antecedent(np.arange(0, 100, 10), 'sentencelength')\n",
    "numtokens = ctrl.Antecedent(np.arange(0, 100, 10), 'numtokens')\n",
    "keywords = ctrl.Antecedent(np.arange(0, 10, 1), 'keywords')\n",
    "tf_isf = ctrl.Antecedent(np.arange(0, 100, 10), 'tf_isf')\n",
    "\n",
    "\n",
    "senten = ctrl.Consequent(np.arange(0, 100, 10), 'senten')\n",
    "\n",
    "position1.automf(3)\n",
    "cos_similarity.automf(3)\n",
    "bitokens.automf(3)\n",
    "tritokens.automf(3)\n",
    "propernoun.automf(3)\n",
    "sentencelength.automf(3)\n",
    "numtokens.automf(3)\n",
    "keywords.automf(3)\n",
    "tf_isf.automf(3)\n",
    "\n",
    "\n",
    "senten['bad'] = fuzz.trimf(senten.universe, [0, 0, 50])\n",
    "senten['avg'] = fuzz.trimf(senten.universe, [0, 50, 100])\n",
    "senten['good'] = fuzz.trimf(senten.universe, [50, 100, 100])\n",
    "\n",
    "rule1 = ctrl.Rule(position1['good'] & sentencelength['good'] & propernoun['good'] &numtokens['good'], senten['good'])\n",
    "rule2 = ctrl.Rule(position1['poor'] & sentencelength['poor'] & numtokens['poor'], senten['bad'])\n",
    "rule3 = ctrl.Rule(propernoun['poor'] & keywords['average'], senten['bad'])\n",
    "rule4 = ctrl.Rule(cos_similarity['good'], senten['good'])\n",
    "rule5 = ctrl.Rule(bitokens['good'] & tritokens['good'] & numtokens['average'] | tf_isf['average'], senten['avg'])\n",
    "\n",
    "\n",
    "sent_ctrl = ctrl.ControlSystem([rule1,rule2,rule3,rule4,rule5])\n",
    "Sent = ctrl.ControlSystemSimulation(sent_ctrl)\n",
    "fuzzemptyarr= np.empty((20,1,2), dtype=object)\n",
    "t2=0\n",
    "summary2=[]\n",
    "for s in range(len(sentences)):\n",
    "    Sent.input['position1'] = int(position[s])\n",
    "    Sent.input['cos_similarity'] = int(cosine_similarity[s])\n",
    "    Sent.input['bitokens'] = int(bi_tokens[s])\n",
    "    Sent.input['tritokens'] = int(tri_tokens[s])\n",
    "    Sent.input['tf_isf'] = int(tfisfvec[s])\n",
    "    Sent.input['keywords'] = int(thematic_number[s])\n",
    "    Sent.input['propernoun'] = int(pnounscore[s])\n",
    "    Sent.input['sentencelength'] = int(sent_length[s])\n",
    "    Sent.input['numtokens'] = int(numeric_token[s])\n",
    "#Sent.input['service'] = 2\n",
    "    Sent.compute()\n",
    "    if Sent.output['senten'] > 50:\n",
    "        summary2.append((sentences[s]))\n",
    "        fuzzemptyarr[t2][0][0] = sentences[s]\n",
    "        fuzzemptyarr[t2][0][1] = s\n",
    "        t2+=1\n",
    "fuzzarray = np.empty((len(summary2),1,2),dtype=object)\n",
    "for i in range(len(summary2)):\n",
    "    fuzzarray[i][0][0] = fuzzemptyarr[i][0][0]\n",
    "    fuzzarray[i][0][1] = fuzzemptyarr[i][0][1]\n",
    "    \n",
    "fuzzarray=fuzzarray[1:]\n",
    "print(\"Fuzzy logic summary \\n\\n\",summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
